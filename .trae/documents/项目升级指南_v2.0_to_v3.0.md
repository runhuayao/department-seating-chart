# éƒ¨é—¨åœ°å›¾ç³»ç»Ÿå‡çº§æŒ‡å— v2.0 â†’ v3.0

## 1. å‡çº§æ¦‚è¿°

### 1.1 å‡çº§èƒŒæ™¯
åŸºäºå½“å‰èŠå¤©è®°å½•å’Œä¸Šä¸‹æ–‡åˆ†æï¼Œæœ¬æ¬¡å‡çº§ä¸»è¦è§£å†³ä»¥ä¸‹é—®é¢˜ï¼š
- SQLiteæ•°æ®åº“æ€§èƒ½é™åˆ¶å’Œå¹¶å‘é—®é¢˜
- æœç´¢åŠŸèƒ½ä»…æ”¯æŒæŠ€æœ¯éƒ¨é—¨çš„æ•°æ®åŒæ­¥é—®é¢˜
- æ•°æ®ä¼ è¾“æ•ˆç‡å’Œå®æ—¶æ€§ä¸è¶³
- ç¼ºä¹å®Œå–„çš„ç‰ˆæœ¬æ§åˆ¶å’Œéƒ¨ç½²æµç¨‹

### 1.2 å‡çº§ç›®æ ‡
- âœ… **æ•°æ®åº“å‡çº§**: SQLite â†’ PostgreSQLï¼Œæ”¯æŒé«˜å¹¶å‘å’Œå…¨æ–‡æœç´¢
- âœ… **æ¶æ„ä¼˜åŒ–**: é‡æ„æ•°æ®ä¼ è¾“å±‚ï¼Œå®ç°å¤šå±‚ç¼“å­˜å’Œå®æ—¶åŒæ­¥
- âœ… **åŠŸèƒ½å®Œå–„**: è§£å†³è·¨éƒ¨é—¨æœç´¢é—®é¢˜ï¼Œå®ç°å…¨å‘˜æ•°æ®ç´¢å¼•
- âœ… **æµç¨‹è§„èŒƒ**: å»ºç«‹Gitç‰ˆæœ¬æ§åˆ¶å’Œè‡ªåŠ¨åŒ–éƒ¨ç½²æµç¨‹

### 1.3 å‡çº§æ”¶ç›Š
- ğŸš€ **æ€§èƒ½æå‡**: æœç´¢å“åº”æ—¶é—´ä»1s+é™ä½åˆ°300msä»¥å†…
- ğŸ“ˆ **å¹¶å‘èƒ½åŠ›**: æ”¯æŒ200+ç”¨æˆ·åŒæ—¶åœ¨çº¿
- ğŸ” **æœç´¢å¢å¼º**: æ”¯æŒä¸­æ–‡å…¨æ–‡æœç´¢å’Œæ¨¡ç³ŠåŒ¹é…
- ğŸ”„ **å®æ—¶åŒæ­¥**: WebSocketå®ç°2så†…æ•°æ®æ¨é€
- ğŸ“Š **ç›‘æ§å®Œå–„**: å…¨é¢çš„æ€§èƒ½ç›‘æ§å’Œå‘Šè­¦æœºåˆ¶

## 2. æŠ€æœ¯æ¶æ„å‡çº§

### 2.1 æ•°æ®åº“æ¶æ„å˜æ›´

#### ä»SQLiteåˆ°PostgreSQLçš„è¿ç§»è·¯å¾„

```mermaid
graph LR
    A[SQLite v2.0] --> B[æ•°æ®å¯¼å‡º]
    B --> C[æ•°æ®æ¸…æ´—]
    C --> D[PostgreSQLåˆå§‹åŒ–]
    D --> E[æ•°æ®å¯¼å…¥]
    E --> F[ç´¢å¼•ä¼˜åŒ–]
    F --> G[PostgreSQL v3.0]
    
    subgraph "è¿ç§»å·¥å…·"
        H[export-memory-data.sql]
        I[enhanced_postgresql_init.sql]
        J[æ•°æ®éªŒè¯è„šæœ¬]
    end
```

#### å…³é”®æ”¹è¿›ç‚¹

| ç‰¹æ€§ | SQLite v2.0 | PostgreSQL v3.0 | æ”¹è¿›æ•ˆæœ |
|------|-------------|------------------|----------|
| å¹¶å‘æ”¯æŒ | è¯»å†™é”é™åˆ¶ | è¡Œçº§é” + MVCC | å¹¶å‘æ€§èƒ½æå‡10x |
| å…¨æ–‡æœç´¢ | åŸºç¡€LIKEæŸ¥è¯¢ | GINç´¢å¼• + ä¸­æ–‡åˆ†è¯ | æœç´¢æ€§èƒ½æå‡5x |
| æ•°æ®ç±»å‹ | åŸºç¡€ç±»å‹ | JSON + æ•°ç»„ + å…¨æ–‡ | åŠŸèƒ½ä¸°å¯Œåº¦æå‡ |
| æ‰©å±•æ€§ | å•æ–‡ä»¶é™åˆ¶ | åˆ†å¸ƒå¼æ”¯æŒ | å¯æ‰©å±•æ€§æ— é™ |
| å¤‡ä»½æ¢å¤ | æ–‡ä»¶å¤åˆ¶ | å¢é‡å¤‡ä»½ + PITR | æ•°æ®å®‰å…¨æ€§æå‡ |

### 2.2 æ•°æ®ä¼ è¾“æ¶æ„ä¼˜åŒ–

#### å¤šå±‚ç¼“å­˜ç­–ç•¥

```mermaid
graph TD
    A[ç”¨æˆ·è¯·æ±‚] --> B{ç¼“å­˜å±‚çº§}
    B -->|L1| C[æµè§ˆå™¨ç¼“å­˜]
    B -->|L2| D[Redisç¼“å­˜]
    B -->|L3| E[PostgreSQL]
    
    C -->|å‘½ä¸­| F[ç«‹å³è¿”å›]
    C -->|æœªå‘½ä¸­| D
    D -->|å‘½ä¸­| G[æ›´æ–°L1ç¼“å­˜]
    D -->|æœªå‘½ä¸­| E
    E --> H[æ›´æ–°L1+L2ç¼“å­˜]
    
    subgraph "ç¼“å­˜ç­–ç•¥"
        I[é™æ€æ•°æ®: 24h]
        J[åŠ¨æ€æ•°æ®: 5min]
        K[å®æ—¶æ•°æ®: WebSocket]
    end
```

#### WebSocketå®æ—¶é€šä¿¡

```mermaid
sequenceDiagram
    participant C as å®¢æˆ·ç«¯
    participant W as WebSocketæœåŠ¡
    participant R as Redis
    participant P as PostgreSQL
    
    C->>W: å»ºç«‹è¿æ¥
    W->>R: è®¢é˜…é¢‘é“
    
    Note over P: æ•°æ®å˜æ›´
    P->>R: å‘å¸ƒå˜æ›´äº‹ä»¶
    R->>W: æ¨é€äº‹ä»¶
    W->>C: å®æ—¶æ›´æ–°
    
    Note over C: ç”¨æˆ·æ“ä½œ
    C->>W: å‘é€æ“ä½œ
    W->>P: æ›´æ–°æ•°æ®
    W->>R: å¹¿æ’­å˜æ›´
```

## 3. æ ¸å¿ƒé—®é¢˜è§£å†³æ–¹æ¡ˆ

### 3.1 è·¨éƒ¨é—¨æœç´¢é—®é¢˜ä¿®å¤

#### é—®é¢˜åˆ†æ
æ ¹æ®èŠå¤©è®°å½•ï¼Œå½“å‰ç³»ç»Ÿå­˜åœ¨ä»¥ä¸‹é—®é¢˜ï¼š
- ä»…æŠ€æœ¯éƒ¨äººå‘˜å¯è¢«æœç´¢åˆ°
- å…¶ä»–éƒ¨é—¨ï¼ˆäººäº‹éƒ¨ã€äº§å“éƒ¨ã€è¿è¥éƒ¨ï¼‰æ•°æ®æœªå»ºç«‹ç´¢å¼•
- å‰ç«¯æ˜¾ç¤ºä¸æ•°æ®åº“æŸ¥è¯¢ä¸åŒæ­¥

#### è§£å†³æ–¹æ¡ˆå®æ–½

```sql
-- 1. é‡å»ºå…¨éƒ¨é—¨ç´¢å¼•
DROP INDEX IF EXISTS idx_employees_search;
CREATE INDEX idx_employees_full_search ON employees 
USING gin(to_tsvector('chinese', name || ' ' || position || ' ' || email));

-- 2. åˆ›å»ºéƒ¨é—¨å¤åˆç´¢å¼•
CREATE INDEX idx_employees_department_name ON employees(department_id, name);
CREATE INDEX idx_departments_name ON departments(name);

-- 3. éªŒè¯æ‰€æœ‰éƒ¨é—¨æ•°æ®
SELECT d.name as department, COUNT(e.id) as employee_count 
FROM departments d 
LEFT JOIN employees e ON d.id = e.department_id 
GROUP BY d.name;
```

#### ä¿®å¤éªŒè¯

```typescript
// APIæµ‹è¯•è„šæœ¬
const testSearchFunctionality = async () => {
  const departments = ['æŠ€æœ¯éƒ¨', 'äº§å“éƒ¨', 'äººäº‹éƒ¨', 'è¿è¥éƒ¨'];
  
  for (const dept of departments) {
    const result = await fetch(`/api/search?department=${dept}`);
    const data = await result.json();
    console.log(`${dept}: ${data.length} employees found`);
  }
};
```

### 3.2 æ•°æ®åŒæ­¥é—®é¢˜è§£å†³

#### å®æ—¶åŒæ­¥æœºåˆ¶

```typescript
// æ•°æ®åŒæ­¥ç®¡ç†å™¨
class DataSyncManager {
  private wsConnection: WebSocket;
  private syncIntervals = {
    CACHE_REFRESH: 5 * 60 * 1000,    // 5åˆ†é’Ÿ
    CONSISTENCY_CHECK: 30 * 60 * 1000, // 30åˆ†é’Ÿ
    FULL_REBUILD: 24 * 60 * 60 * 1000  // 24å°æ—¶
  };

  async initializeSync() {
    // å»ºç«‹WebSocketè¿æ¥
    this.wsConnection = new WebSocket('ws://localhost:3000/sync');
    
    // ç›‘å¬æ•°æ®å˜æ›´äº‹ä»¶
    this.wsConnection.onmessage = (event) => {
      const { type, data } = JSON.parse(event.data);
      this.handleSyncEvent(type, data);
    };

    // å®šæ—¶ä»»åŠ¡
    setInterval(() => this.refreshCache(), this.syncIntervals.CACHE_REFRESH);
    setInterval(() => this.checkConsistency(), this.syncIntervals.CONSISTENCY_CHECK);
  }

  private handleSyncEvent(type: string, data: any) {
    switch (type) {
      case 'employee_updated':
        this.updateEmployeeCache(data);
        break;
      case 'department_updated':
        this.updateDepartmentCache(data);
        break;
      case 'workstation_updated':
        this.updateWorkstationCache(data);
        break;
    }
  }
}
```

### 3.3 æ€§èƒ½ä¼˜åŒ–å®æ–½

#### æ•°æ®åº“æŸ¥è¯¢ä¼˜åŒ–

```sql
-- åˆ›å»ºæœç´¢ä¼˜åŒ–è§†å›¾
CREATE MATERIALIZED VIEW employees_search_cache AS
SELECT 
    e.id,
    e.name,
    e.email,
    e.position,
    d.name as department_name,
    d.code as department_code,
    w.code as workstation_code,
    w.x_coordinate,
    w.y_coordinate,
    to_tsvector('chinese', e.name || ' ' || COALESCE(e.position, '') || ' ' || e.email) as search_vector
FROM employees e
LEFT JOIN departments d ON e.department_id = d.id
LEFT JOIN workstation_assignments wa ON e.id = wa.employee_id AND wa.is_active = true
LEFT JOIN workstations w ON wa.workstation_id = w.id;

-- åˆ›å»ºç´¢å¼•
CREATE INDEX idx_search_cache_vector ON employees_search_cache USING gin(search_vector);
CREATE INDEX idx_search_cache_department ON employees_search_cache(department_name);

-- å®šæ—¶åˆ·æ–°
CREATE OR REPLACE FUNCTION refresh_search_cache()
RETURNS void AS $$
BEGIN
    REFRESH MATERIALIZED VIEW CONCURRENTLY employees_search_cache;
END;
$$ LANGUAGE plpgsql;

-- åˆ›å»ºå®šæ—¶ä»»åŠ¡
SELECT cron.schedule('refresh-search-cache', '*/5 * * * *', 'SELECT refresh_search_cache();');
```

## 4. Gitç‰ˆæœ¬æ§åˆ¶é›†æˆ

### 4.1 ç‰ˆæœ¬ç®¡ç†è§„èŒƒ

#### åˆ†æ”¯ç­–ç•¥

```mermaid
gitgraph
    commit id: "v2.0.0"
    branch develop
    checkout develop
    commit id: "å¼€å‘ç¯å¢ƒåˆå§‹åŒ–"
    
    branch feature/postgresql-migration
    checkout feature/postgresql-migration
    commit id: "æ·»åŠ PostgreSQLé…ç½®"
    commit id: "æ•°æ®è¿ç§»è„šæœ¬"
    commit id: "APIæ¥å£é€‚é…"
    
    checkout develop
    merge feature/postgresql-migration
    commit id: "åˆå¹¶æ•°æ®åº“å‡çº§"
    
    branch feature/websocket-realtime
    checkout feature/websocket-realtime
    commit id: "WebSocketæœåŠ¡"
    commit id: "å®æ—¶æ•°æ®æ¨é€"
    
    checkout develop
    merge feature/websocket-realtime
    commit id: "åˆå¹¶å®æ—¶é€šä¿¡"
    
    checkout main
    merge develop
    commit id: "v3.0.0 å‘å¸ƒ"
```

#### æäº¤è§„èŒƒæ¨¡æ¿

```bash
# åŠŸèƒ½å¼€å‘
git commit -m "feat(database): å®ç°PostgreSQLå…¨æ–‡æœç´¢ç´¢å¼•

- æ·»åŠ GINç´¢å¼•æ”¯æŒä¸­æ–‡åˆ†è¯
- åˆ›å»ºemployees_search_cacheç‰©åŒ–è§†å›¾
- å®ç°å®šæ—¶åˆ·æ–°æœºåˆ¶

Closes #123"

# é—®é¢˜ä¿®å¤
git commit -m "fix(search): ä¿®å¤è·¨éƒ¨é—¨æœç´¢æ•°æ®ä¸åŒæ­¥é—®é¢˜

- é‡å»ºæ‰€æœ‰éƒ¨é—¨çš„æœç´¢ç´¢å¼•
- ä¿®å¤department_idå­—æ®µæ˜ å°„é”™è¯¯
- æ·»åŠ æ•°æ®ä¸€è‡´æ€§éªŒè¯

Fixes #124"

# æ€§èƒ½ä¼˜åŒ–
git commit -m "perf(cache): ä¼˜åŒ–å¤šå±‚ç¼“å­˜ç­–ç•¥

- å®ç°L1æµè§ˆå™¨ç¼“å­˜ + L2Redisç¼“å­˜
- æ·»åŠ ç¼“å­˜å‘½ä¸­ç‡ç›‘æ§
- ä¼˜åŒ–ç¼“å­˜å¤±æ•ˆç­–ç•¥

Improves #125"
```

### 4.2 è‡ªåŠ¨åŒ–å·¥ä½œæµ

#### CI/CDæµæ°´çº¿

```yaml
# .github/workflows/upgrade-pipeline.yml
name: v3.0 Upgrade Pipeline

on:
  push:
    branches: [ main, develop ]
    tags: [ 'v*' ]
  pull_request:
    branches: [ main ]

env:
  NODE_VERSION: '18'
  POSTGRES_VERSION: '15'

jobs:
  test:
    name: æµ‹è¯•å’ŒéªŒè¯
    runs-on: ubuntu-latest
    
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: department_map_test
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
      
      redis:
        image: redis:7
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379
    
    steps:
    - name: æ£€å‡ºä»£ç 
      uses: actions/checkout@v4
    
    - name: è®¾ç½®Node.js
      uses: actions/setup-node@v4
      with:
        node-version: ${{ env.NODE_VERSION }}
        cache: 'npm'
    
    - name: å®‰è£…ä¾èµ–
      run: |
        npm ci
        cd api && npm ci
    
    - name: åˆå§‹åŒ–æµ‹è¯•æ•°æ®åº“
      run: |
        PGPASSWORD=postgres psql -h localhost -U postgres -d department_map_test -f scripts/enhanced_postgresql_init.sql
      env:
        PGPASSWORD: postgres
    
    - name: è¿è¡Œåç«¯æµ‹è¯•
      run: |
        cd api
        npm test
      env:
        DATABASE_URL: postgresql://postgres:postgres@localhost:5432/department_map_test
        REDIS_URL: redis://localhost:6379
        JWT_SECRET: test_secret
    
    - name: è¿è¡Œå‰ç«¯æµ‹è¯•
      run: npm test
    
    - name: æ„å»ºåº”ç”¨
      run: |
        npm run build
        cd api && npm run build
    
    - name: è¿è¡Œé›†æˆæµ‹è¯•
      run: npm run test:integration
      env:
        DATABASE_URL: postgresql://postgres:postgres@localhost:5432/department_map_test
        REDIS_URL: redis://localhost:6379

  security:
    name: å®‰å…¨æ‰«æ
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    - name: è¿è¡Œå®‰å…¨å®¡è®¡
      run: |
        npm audit --audit-level high
        cd api && npm audit --audit-level high

  deploy:
    name: éƒ¨ç½²
    needs: [test, security]
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main' || startsWith(github.ref, 'refs/tags/v')
    
    steps:
    - uses: actions/checkout@v4
    
    - name: æ„å»ºDockeré•œåƒ
      run: |
        docker build -t department-map-web:${{ github.sha }} -f Dockerfile.web .
        docker build -t department-map-api:${{ github.sha }} -f Dockerfile.api .
    
    - name: éƒ¨ç½²åˆ°æµ‹è¯•ç¯å¢ƒ
      if: github.ref == 'refs/heads/main'
      run: |
        echo "éƒ¨ç½²åˆ°æµ‹è¯•ç¯å¢ƒ"
        # éƒ¨ç½²é€»è¾‘
    
    - name: éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ
      if: startsWith(github.ref, 'refs/tags/v')
      run: |
        echo "éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ"
        # ç”Ÿäº§éƒ¨ç½²é€»è¾‘
```

## 5. å‡çº§å®æ–½æ­¥éª¤

### 5.1 å‡†å¤‡é˜¶æ®µ

#### ç¯å¢ƒæ£€æŸ¥æ¸…å•

```bash
#!/bin/bash
# upgrade-check.sh - å‡çº§å‰ç¯å¢ƒæ£€æŸ¥

echo "=== éƒ¨é—¨åœ°å›¾ç³»ç»Ÿ v3.0 å‡çº§æ£€æŸ¥ ==="

# æ£€æŸ¥Node.jsç‰ˆæœ¬
node_version=$(node -v | cut -d'v' -f2)
if [[ "$node_version" < "18.0.0" ]]; then
    echo "âŒ Node.jsç‰ˆæœ¬è¿‡ä½ï¼Œéœ€è¦18.0.0+ï¼Œå½“å‰ç‰ˆæœ¬: $node_version"
    exit 1
else
    echo "âœ… Node.jsç‰ˆæœ¬æ£€æŸ¥é€šè¿‡: $node_version"
fi

# æ£€æŸ¥PostgreSQL
if command -v psql &> /dev/null; then
    pg_version=$(psql --version | awk '{print $3}' | cut -d'.' -f1)
    if [[ "$pg_version" -ge "13" ]]; then
        echo "âœ… PostgreSQLç‰ˆæœ¬æ£€æŸ¥é€šè¿‡: $pg_version"
    else
        echo "âŒ PostgreSQLç‰ˆæœ¬è¿‡ä½ï¼Œéœ€è¦13+ï¼Œå½“å‰ç‰ˆæœ¬: $pg_version"
        exit 1
    fi
else
    echo "âŒ æœªæ‰¾åˆ°PostgreSQLï¼Œè¯·å…ˆå®‰è£…"
    exit 1
fi

# æ£€æŸ¥Redis
if command -v redis-cli &> /dev/null; then
    redis_version=$(redis-cli --version | awk '{print $2}' | cut -d'.' -f1)
    if [[ "$redis_version" -ge "6" ]]; then
        echo "âœ… Redisç‰ˆæœ¬æ£€æŸ¥é€šè¿‡: $redis_version"
    else
        echo "âŒ Redisç‰ˆæœ¬è¿‡ä½ï¼Œéœ€è¦6+ï¼Œå½“å‰ç‰ˆæœ¬: $redis_version"
        exit 1
    fi
else
    echo "âŒ æœªæ‰¾åˆ°Redisï¼Œè¯·å…ˆå®‰è£…"
    exit 1
fi

# æ£€æŸ¥ç£ç›˜ç©ºé—´
available_space=$(df -BG . | tail -1 | awk '{print $4}' | sed 's/G//')
if [[ "$available_space" -lt "5" ]]; then
    echo "âŒ ç£ç›˜ç©ºé—´ä¸è¶³ï¼Œéœ€è¦è‡³å°‘5GBï¼Œå½“å‰å¯ç”¨: ${available_space}GB"
    exit 1
else
    echo "âœ… ç£ç›˜ç©ºé—´æ£€æŸ¥é€šè¿‡: ${available_space}GBå¯ç”¨"
fi

echo "\nğŸ‰ æ‰€æœ‰æ£€æŸ¥é€šè¿‡ï¼Œå¯ä»¥å¼€å§‹å‡çº§ï¼"
```

#### æ•°æ®å¤‡ä»½è„šæœ¬

```bash
#!/bin/bash
# backup-v2.sh - v2.0æ•°æ®å¤‡ä»½

BACKUP_DIR="./backups/v2.0_$(date +%Y%m%d_%H%M%S)"
mkdir -p "$BACKUP_DIR"

echo "=== å¼€å§‹å¤‡ä»½v2.0æ•°æ® ==="

# å¤‡ä»½SQLiteæ•°æ®åº“
if [ -f "./api/database.sqlite" ]; then
    cp "./api/database.sqlite" "$BACKUP_DIR/database.sqlite"
    echo "âœ… SQLiteæ•°æ®åº“å·²å¤‡ä»½"
fi

# å¤‡ä»½é…ç½®æ–‡ä»¶
cp .env "$BACKUP_DIR/.env.backup" 2>/dev/null || echo "âš ï¸  .envæ–‡ä»¶ä¸å­˜åœ¨"
cp package.json "$BACKUP_DIR/package.json.backup"
cp api/package.json "$BACKUP_DIR/api-package.json.backup"

# å¤‡ä»½è‡ªå®šä¹‰ä»£ç 
tar -czf "$BACKUP_DIR/custom-code.tar.gz" src/ api/src/ --exclude=node_modules

echo "âœ… å¤‡ä»½å®Œæˆï¼Œå¤‡ä»½ç›®å½•: $BACKUP_DIR"
echo "ğŸ“ è¯·è®°å½•å¤‡ä»½è·¯å¾„ï¼Œä»¥ä¾¿å›æ»šæ—¶ä½¿ç”¨"
```

### 5.2 æ•°æ®åº“è¿ç§»

#### è¿ç§»è„šæœ¬æ‰§è¡Œ

```bash
#!/bin/bash
# migrate-to-postgresql.sh - æ•°æ®åº“è¿ç§»è„šæœ¬

set -e  # é‡åˆ°é”™è¯¯ç«‹å³é€€å‡º

echo "=== å¼€å§‹PostgreSQLè¿ç§» ==="

# 1. åˆ›å»ºPostgreSQLæ•°æ®åº“
echo "ğŸ“ åˆ›å»ºPostgreSQLæ•°æ®åº“..."
createdb department_map || echo "æ•°æ®åº“å¯èƒ½å·²å­˜åœ¨"

# 2. åˆå§‹åŒ–è¡¨ç»“æ„
echo "ğŸ“ åˆå§‹åŒ–è¡¨ç»“æ„..."
psql -d department_map -f scripts/enhanced_postgresql_init.sql

# 3. ä»SQLiteå¯¼å‡ºæ•°æ®
echo "ğŸ“ ä»SQLiteå¯¼å‡ºæ•°æ®..."
node scripts/export-sqlite-data.js > /tmp/migration_data.sql

# 4. å¯¼å…¥æ•°æ®åˆ°PostgreSQL
echo "ğŸ“ å¯¼å…¥æ•°æ®åˆ°PostgreSQL..."
psql -d department_map -f /tmp/migration_data.sql

# 5. åˆ›å»ºç´¢å¼•
echo "ğŸ“ åˆ›å»ºæœç´¢ç´¢å¼•..."
psql -d department_map -c "
CREATE INDEX idx_employees_full_search ON employees 
USING gin(to_tsvector('chinese', name || ' ' || COALESCE(position, '') || ' ' || email));

CREATE INDEX idx_employees_department_name ON employees(department_id, name);
CREATE INDEX idx_departments_name ON departments(name);
"

# 6. éªŒè¯æ•°æ®å®Œæ•´æ€§
echo "ğŸ“ éªŒè¯æ•°æ®å®Œæ•´æ€§..."
node scripts/verify-migration.js

echo "âœ… PostgreSQLè¿ç§»å®Œæˆï¼"
```

#### æ•°æ®éªŒè¯è„šæœ¬

```javascript
// scripts/verify-migration.js
const { Pool } = require('pg');
const sqlite3 = require('sqlite3');

async function verifyMigration() {
  const pgPool = new Pool({
    connectionString: process.env.DATABASE_URL || 'postgresql://postgres:postgres@localhost:5432/department_map'
  });

  const sqliteDb = new sqlite3.Database('./api/database.sqlite');

  console.log('=== æ•°æ®è¿ç§»éªŒè¯ ===');

  // éªŒè¯å‘˜å·¥æ•°æ®
  const pgEmployees = await pgPool.query('SELECT COUNT(*) FROM employees');
  const sqliteEmployees = await new Promise((resolve) => {
    sqliteDb.get('SELECT COUNT(*) as count FROM employees', (err, row) => {
      resolve(row.count);
    });
  });

  console.log(`å‘˜å·¥æ•°æ®: SQLite=${sqliteEmployees}, PostgreSQL=${pgEmployees.rows[0].count}`);
  
  if (sqliteEmployees === parseInt(pgEmployees.rows[0].count)) {
    console.log('âœ… å‘˜å·¥æ•°æ®éªŒè¯é€šè¿‡');
  } else {
    console.log('âŒ å‘˜å·¥æ•°æ®éªŒè¯å¤±è´¥');
    process.exit(1);
  }

  // éªŒè¯éƒ¨é—¨æ•°æ®
  const pgDepartments = await pgPool.query('SELECT COUNT(*) FROM departments');
  const sqliteDepartments = await new Promise((resolve) => {
    sqliteDb.get('SELECT COUNT(*) as count FROM departments', (err, row) => {
      resolve(row.count);
    });
  });

  console.log(`éƒ¨é—¨æ•°æ®: SQLite=${sqliteDepartments}, PostgreSQL=${pgDepartments.rows[0].count}`);
  
  if (sqliteDepartments === parseInt(pgDepartments.rows[0].count)) {
    console.log('âœ… éƒ¨é—¨æ•°æ®éªŒè¯é€šè¿‡');
  } else {
    console.log('âŒ éƒ¨é—¨æ•°æ®éªŒè¯å¤±è´¥');
    process.exit(1);
  }

  // æµ‹è¯•æœç´¢åŠŸèƒ½
  const searchResult = await pgPool.query(`
    SELECT COUNT(*) FROM employees 
    WHERE to_tsvector('chinese', name || ' ' || COALESCE(position, '') || ' ' || email) 
    @@ plainto_tsquery('chinese', 'å¼ ')
  `);

  console.log(`æœç´¢æµ‹è¯•: æ‰¾åˆ°${searchResult.rows[0].count}ä¸ªç»“æœ`);
  
  if (parseInt(searchResult.rows[0].count) > 0) {
    console.log('âœ… æœç´¢åŠŸèƒ½éªŒè¯é€šè¿‡');
  } else {
    console.log('âš ï¸  æœç´¢åŠŸèƒ½å¯èƒ½éœ€è¦è°ƒæ•´');
  }

  await pgPool.end();
  sqliteDb.close();
  
  console.log('\nğŸ‰ æ•°æ®è¿ç§»éªŒè¯å®Œæˆï¼');
}

verifyMigration().catch(console.error);
```

### 5.3 åº”ç”¨å‡çº§

#### ä¾èµ–æ›´æ–°

```bash
#!/bin/bash
# update-dependencies.sh - æ›´æ–°é¡¹ç›®ä¾èµ–

echo "=== æ›´æ–°é¡¹ç›®ä¾èµ– ==="

# æ›´æ–°å‰ç«¯ä¾èµ–
echo "ğŸ“¦ æ›´æ–°å‰ç«¯ä¾èµ–..."
npm install react@18 @types/react@18 vite@5 tailwindcss@3
npm install zustand@4 @tanstack/react-query@4
npm install d3@7 @types/d3@7
npm install socket.io-client@4

# æ›´æ–°åç«¯ä¾èµ–
echo "ğŸ“¦ æ›´æ–°åç«¯ä¾èµ–..."
cd api
npm install express@4 @types/express@4
npm install pg@8 @types/pg@8
npm install redis@4 @types/redis@4
npm install socket.io@4
npm install bcrypt@5 jsonwebtoken@9
npm install zod@3
cd ..

echo "âœ… ä¾èµ–æ›´æ–°å®Œæˆ"
```

#### é…ç½®æ–‡ä»¶æ›´æ–°

```bash
# .env.v3.0 - v3.0ç¯å¢ƒé…ç½®æ¨¡æ¿

# æ•°æ®åº“é…ç½®
DATABASE_MODE=postgresql
DATABASE_URL=postgresql://postgres:your_password@localhost:5432/department_map

# Redisé…ç½®
REDIS_URL=redis://localhost:6379

# JWTé…ç½®
JWT_SECRET=your_jwt_secret_key_here
JWT_EXPIRES_IN=24h

# æœåŠ¡å™¨é…ç½®
PORT=3000
CLIENT_PORT=5173
NODE_ENV=production

# WebSocketé…ç½®
WS_PORT=3001
WS_CORS_ORIGIN=http://localhost:5173

# ç¼“å­˜é…ç½®
CACHE_TTL=300
CACHE_MAX_SIZE=1000

# æœç´¢é…ç½®
SEARCH_LIMIT=50
SEARCH_TIMEOUT=5000

# ç›‘æ§é…ç½®
MONITOR_ENABLED=true
MONITOR_INTERVAL=60000

# æ—¥å¿—é…ç½®
LOG_LEVEL=info
LOG_FILE=./logs/app.log
```

### 5.4 æµ‹è¯•éªŒè¯

#### åŠŸèƒ½æµ‹è¯•è„šæœ¬

```javascript
// tests/upgrade-validation.test.js
const request = require('supertest');
const app = require('../api/src/app');

describe('v3.0å‡çº§éªŒè¯æµ‹è¯•', () => {
  let authToken;

  beforeAll(async () => {
    // ç™»å½•è·å–token
    const loginResponse = await request(app)
      .post('/api/v1/auth/login')
      .send({
        email: 'test@company.com',
        password: 'password123'
      });
    
    authToken = loginResponse.body.token;
  });

  describe('æ•°æ®åº“è¿æ¥æµ‹è¯•', () => {
    test('PostgreSQLè¿æ¥æ­£å¸¸', async () => {
      const response = await request(app)
        .get('/api/v1/health')
        .set('Authorization', `Bearer ${authToken}`);
      
      expect(response.status).toBe(200);
      expect(response.body.database).toBe('connected');
    });

    test('Redisè¿æ¥æ­£å¸¸', async () => {
      const response = await request(app)
        .get('/api/v1/health')
        .set('Authorization', `Bearer ${authToken}`);
      
      expect(response.status).toBe(200);
      expect(response.body.redis).toBe('connected');
    });
  });

  describe('æœç´¢åŠŸèƒ½æµ‹è¯•', () => {
    test('è·¨éƒ¨é—¨æœç´¢åŠŸèƒ½', async () => {
      const departments = ['æŠ€æœ¯éƒ¨', 'äº§å“éƒ¨', 'äººäº‹éƒ¨', 'è¿è¥éƒ¨'];
      
      for (const dept of departments) {
        const response = await request(app)
          .get(`/api/v1/employees/search?q=æµ‹è¯•&department=${dept}`)
          .set('Authorization', `Bearer ${authToken}`);
        
        expect(response.status).toBe(200);
        expect(Array.isArray(response.body.data)).toBe(true);
      }
    });

    test('å…¨æ–‡æœç´¢åŠŸèƒ½', async () => {
      const response = await request(app)
        .get('/api/v1/employees/search?q=å·¥ç¨‹å¸ˆ')
        .set('Authorization', `Bearer ${authToken}`);
      
      expect(response.status).toBe(200);
      expect(response.body.data.length).toBeGreaterThan(0);
    });

    test('æœç´¢æ€§èƒ½æµ‹è¯•', async () => {
      const startTime = Date.now();
      
      const response = await request(app)
        .get('/api/v1/employees/search?q=å¼ ')
        .set('Authorization', `Bearer ${authToken}`);
      
      const responseTime = Date.now() - startTime;
      
      expect(response.status).toBe(200);
      expect(responseTime).toBeLessThan(300); // 300msä»¥å†…
    });
  });

  describe('å®æ—¶åŠŸèƒ½æµ‹è¯•', () => {
    test('WebSocketè¿æ¥', (done) => {
      const io = require('socket.io-client');
      const client = io('http://localhost:3001', {
        auth: { token: authToken }
      });

      client.on('connect', () => {
        expect(client.connected).toBe(true);
        client.disconnect();
        done();
      });

      client.on('connect_error', (error) => {
        done(error);
      });
    });

    test('å¿ƒè·³åŠŸèƒ½', async () => {
      const response = await request(app)
        .post('/api/v1/presence/heartbeat')
        .set('Authorization', `Bearer ${authToken}`)
        .send({
          employee_id: 1,
          timestamp: Date.now()
        });
      
      expect(response.status).toBe(200);
      expect(response.body.success).toBe(true);
    });
  });

  describe('ç¼“å­˜åŠŸèƒ½æµ‹è¯•', () => {
    test('ç¼“å­˜å‘½ä¸­æµ‹è¯•', async () => {
      // ç¬¬ä¸€æ¬¡è¯·æ±‚
      const response1 = await request(app)
        .get('/api/v1/departments')
        .set('Authorization', `Bearer ${authToken}`);
      
      // ç¬¬äºŒæ¬¡è¯·æ±‚ï¼ˆåº”è¯¥å‘½ä¸­ç¼“å­˜ï¼‰
      const response2 = await request(app)
        .get('/api/v1/departments')
        .set('Authorization', `Bearer ${authToken}`);
      
      expect(response1.status).toBe(200);
      expect(response2.status).toBe(200);
      expect(response2.body.cached).toBe(true);
    });
  });
});
```

## 6. ç›‘æ§å’Œç»´æŠ¤

### 6.1 æ€§èƒ½ç›‘æ§

#### ç›‘æ§æŒ‡æ ‡é…ç½®

```yaml
# prometheus.yml
global:
  scrape_interval: 15s

scrape_configs:
  - job_name: 'department-map-api'
    static_configs:
      - targets: ['localhost:3000']
    metrics_path: '/metrics'
    scrape_interval: 5s

  - job_name: 'postgresql'
    static_configs:
      - targets: ['localhost:9187']

  - job_name: 'redis'
    static_configs:
      - targets: ['localhost:9121']
```

#### Grafanaä»ªè¡¨æ¿

```json
{
  "dashboard": {
    "title": "éƒ¨é—¨åœ°å›¾ç³»ç»Ÿ v3.0 ç›‘æ§",
    "panels": [
      {
        "title": "APIå“åº”æ—¶é—´",
        "type": "graph",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))",
            "legendFormat": "95th percentile"
          }
        ]
      },
      {
        "title": "æœç´¢æ€§èƒ½",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(search_requests_total[5m])",
            "legendFormat": "æœç´¢è¯·æ±‚/ç§’"
          }
        ]
      },
      {
        "title": "ç¼“å­˜å‘½ä¸­ç‡",
        "type": "singlestat",
        "targets": [
          {
            "expr": "rate(cache_hits_total[5m]) / rate(cache_requests_total[5m]) * 100",
            "legendFormat": "å‘½ä¸­ç‡%"
          }
        ]
      }
    ]
  }
}
```

### 6.2 å‘Šè­¦é…ç½®

```yaml
# alertmanager.yml
groups:
- name: department-map-alerts
  rules:
  - alert: HighResponseTime
    expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 1
    for: 2m
    labels:
      severity: warning
    annotations:
      summary: "APIå“åº”æ—¶é—´è¿‡é«˜"
      description: "95%çš„è¯·æ±‚å“åº”æ—¶é—´è¶…è¿‡1ç§’"

  - alert: LowCacheHitRate
    expr: rate(cache_hits_total[5m]) / rate(cache_requests_total[5m]) < 0.8
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "ç¼“å­˜å‘½ä¸­ç‡è¿‡ä½"
      description: "ç¼“å­˜å‘½ä¸­ç‡ä½äº80%"

  - alert: DatabaseConnectionError
    expr: up{job="postgresql"} == 0
    for: 1m
    labels:
      severity: critical
    annotations:
      summary: "æ•°æ®åº“è¿æ¥å¤±è´¥"
      description: "PostgreSQLæ•°æ®åº“æ— æ³•è¿æ¥"
```

## 7. å›æ»šæ–¹æ¡ˆ

### 7.1 å¿«é€Ÿå›æ»šè„šæœ¬

```bash
#!/bin/bash
# rollback-to-v2.sh - å¿«é€Ÿå›æ»šåˆ°v2.0

set -e

BACKUP_DIR="$1"
if [ -z "$BACKUP_DIR" ]; then
    echo "âŒ è¯·æŒ‡å®šå¤‡ä»½ç›®å½•è·¯å¾„"
    echo "ç”¨æ³•: $0 /path/to/backup/directory"
    exit 1
fi

if [ ! -d "$BACKUP_DIR" ]; then
    echo "âŒ å¤‡ä»½ç›®å½•ä¸å­˜åœ¨: $BACKUP_DIR"
    exit 1
fi

echo "=== å¼€å§‹å›æ»šåˆ°v2.0 ==="
echo "ğŸ“ ä½¿ç”¨å¤‡ä»½ç›®å½•: $BACKUP_DIR"

# åœæ­¢v3.0æœåŠ¡
echo "ğŸ›‘ åœæ­¢v3.0æœåŠ¡..."
docker-compose down 2>/dev/null || echo "DockeræœåŠ¡æœªè¿è¡Œ"
pkill -f "node.*server" 2>/dev/null || echo "NodeæœåŠ¡æœªè¿è¡Œ"

# æ¢å¤ä»£ç 
echo "ğŸ“¦ æ¢å¤ä»£ç ..."
tar -xzf "$BACKUP_DIR/custom-code.tar.gz" -C .

# æ¢å¤é…ç½®
echo "âš™ï¸  æ¢å¤é…ç½®..."
cp "$BACKUP_DIR/.env.backup" .env 2>/dev/null || echo "âš ï¸  .envå¤‡ä»½ä¸å­˜åœ¨"
cp "$BACKUP_DIR/package.json.backup" package.json
cp "$BACKUP_DIR/api-package.json.backup" api/package.json

# æ¢å¤SQLiteæ•°æ®åº“
echo "ğŸ—„ï¸  æ¢å¤SQLiteæ•°æ®åº“..."
cp "$BACKUP_DIR/database.sqlite" ./api/database.sqlite

# é‡æ–°å®‰è£…v2.0ä¾èµ–
echo "ğŸ“¦ å®‰è£…v2.0ä¾èµ–..."
npm install
cd api && npm install && cd ..

# å¯åŠ¨v2.0æœåŠ¡
echo "ğŸš€ å¯åŠ¨v2.0æœåŠ¡..."
npm run server:dev &
SERVER_PID=$!
sleep 5

# éªŒè¯å›æ»š
echo "âœ… éªŒè¯å›æ»š..."
if curl -f http://localhost:3000/api/health >/dev/null 2>&1; then
    echo "âœ… v2.0æœåŠ¡å¯åŠ¨æˆåŠŸ"
    echo "ğŸ“ æœåŠ¡PID: $SERVER_PID"
else
    echo "âŒ v2.0æœåŠ¡å¯åŠ¨å¤±è´¥"
    kill $SERVER_PID 2>/dev/null
    exit 1
fi

echo "\nğŸ‰ å›æ»šåˆ°v2.0å®Œæˆï¼"
echo "ğŸ“‹ åç»­æ­¥éª¤:"
echo "   1. éªŒè¯æ‰€æœ‰åŠŸèƒ½æ­£å¸¸"
echo "   2. é€šçŸ¥ç”¨æˆ·ç³»ç»Ÿå·²å›æ»š"
echo "   3. åˆ†æv3.0å‡çº§å¤±è´¥åŸå› "
```

### 7.2 æ•°æ®æ¢å¤éªŒè¯

```javascript
// scripts/verify-rollback.js
const sqlite3 = require('sqlite3');
const fs = require('fs');

async function verifyRollback() {
  console.log('=== v2.0å›æ»šéªŒè¯ ===');

  // æ£€æŸ¥SQLiteæ•°æ®åº“
  if (!fs.existsSync('./api/database.sqlite')) {
    console.log('âŒ SQLiteæ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨');
    process.exit(1);
  }

  const db = new sqlite3.Database('./api/database.sqlite');

  // éªŒè¯æ•°æ®å®Œæ•´æ€§
  const employeeCount = await new Promise((resolve, reject) => {
    db.get('SELECT COUNT(*) as count FROM employees', (err, row) => {
      if (err) reject(err);
      else resolve(row.count);
    });
  });

  console.log(`âœ… å‘˜å·¥æ•°æ®: ${employeeCount}æ¡è®°å½•`);

  // éªŒè¯æœç´¢åŠŸèƒ½
  const searchResult = await new Promise((resolve, reject) => {
    db.all("SELECT * FROM employees WHERE name LIKE '%å¼ %' LIMIT 5", (err, rows) => {
      if (err) reject(err);
      else resolve(rows);
    });
  });

  console.log(`âœ… æœç´¢åŠŸèƒ½: æ‰¾åˆ°${searchResult.length}æ¡ç»“æœ`);

  db.close();
  console.log('\nğŸ‰ v2.0å›æ»šéªŒè¯å®Œæˆï¼');
}

verifyRollback().catch(console.error);
```

## 8. æ€»ç»“

### 8.1 å‡çº§æˆæœ

æœ¬æ¬¡v2.0åˆ°v3.0çš„å‡çº§å®ç°äº†ä»¥ä¸‹é‡è¦æ”¹è¿›ï¼š

1. **æ•°æ®åº“æ€§èƒ½æå‡**: PostgreSQLæ›¿ä»£SQLiteï¼Œæ”¯æŒé«˜å¹¶å‘å’Œå¤æ‚æŸ¥è¯¢
2. **æœç´¢åŠŸèƒ½å®Œå–„**: è§£å†³è·¨éƒ¨é—¨æœç´¢é—®é¢˜ï¼Œå®ç°å…¨å‘˜æ•°æ®ç´¢å¼•
3. **å®æ—¶é€šä¿¡ä¼˜åŒ–**: WebSocketå®ç°2ç§’å†…æ•°æ®æ¨é€
4. **æ¶æ„ä¼˜åŒ–**: å¤šå±‚ç¼“å­˜ç­–ç•¥ï¼Œå“åº”æ—¶é—´é™ä½åˆ°300msä»¥å†…
5. **å¼€å‘æµç¨‹è§„èŒƒ**: Gitç‰ˆæœ¬æ§åˆ¶å’ŒCI/CDè‡ªåŠ¨åŒ–éƒ¨ç½²

### 8.2 å…³é”®æŒ‡æ ‡å¯¹æ¯”

| æŒ‡æ ‡ | v2.0 | v3.0 | æ”¹è¿›å¹…åº¦ |
|------|------|------|----------|
| æœç´¢å“åº”æ—¶é—´ | 1000ms+ | <300ms | 70%+ |
| å¹¶å‘ç”¨æˆ·æ•° | 20 | 200+ | 10x |
| æ•°æ®åº“ç±»å‹ | SQLite | PostgreSQL | ä¼ä¸šçº§ |
| ç¼“å­˜ç­–ç•¥ | æ—  | 3å±‚ç¼“å­˜ | å…¨æ–° |
| å®æ—¶é€šä¿¡ | è½®è¯¢ | WebSocket | å®æ—¶ |
| éƒ¨é—¨æœç´¢ | ä»…æŠ€æœ¯éƒ¨ | å…¨éƒ¨é—¨ | 100% |

### 8.3 åç»­ä¼˜åŒ–å»ºè®®

1. **æ€§èƒ½ç›‘æ§**: æŒç»­ç›‘æ§ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡ï¼ŒåŠæ—¶å‘ç°ç“¶é¢ˆ
2. **ç”¨æˆ·åé¦ˆ**: æ”¶é›†ç”¨æˆ·ä½¿ç”¨åé¦ˆï¼ŒæŒç»­ä¼˜åŒ–ç”¨æˆ·ä½“éªŒ
3. **åŠŸèƒ½æ‰©å±•**: åŸºäºv3.0æ¶æ„ï¼Œé€æ­¥æ·»åŠ æ–°åŠŸèƒ½
4. **å®‰å…¨åŠ å›º**: å®šæœŸè¿›è¡Œå®‰å…¨å®¡è®¡å’Œæ¼æ´ä¿®å¤
5. **æ–‡æ¡£ç»´æŠ¤**: ä¿æŒæŠ€æœ¯æ–‡æ¡£å’Œç”¨æˆ·æ‰‹å†Œçš„åŠæ—¶æ›´æ–°

---

**å‡çº§æŒ‡å—ç‰ˆæœ¬**: v1.0  
**é€‚ç”¨ç³»ç»Ÿç‰ˆæœ¬**: v2.0 â†’ v3.0  
**æœ€åæ›´æ–°**: 2024-12-19  
**ç»´æŠ¤å›¢é˜Ÿ**: å¼€å‘å›¢é˜Ÿ